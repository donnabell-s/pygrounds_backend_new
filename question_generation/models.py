from django.db import models
from django.db.models import JSONField
from django.contrib.postgres.fields import ArrayField
from content_ingestion.models import Topic, Subtopic, DocumentChunk, UploadedDocument


class GeneratedQuestion(models.Model):
    """
    Questions generated by RAG system for specific subtopics.
    """
    # Content relationships
    topic = models.ForeignKey(Topic, on_delete=models.CASCADE, related_name='generated_questions')
    subtopic = models.ForeignKey(Subtopic, on_delete=models.CASCADE, related_name='generated_questions')
    
    # Question content
    question_text = models.TextField(help_text="The actual question text")
    question_type = models.CharField(
        max_length=50,
        choices=[
            ('multiple_choice', 'Multiple Choice'),
            ('code_completion', 'Code Completion'),
            ('true_false', 'True/False'),
            ('short_answer', 'Short Answer'),
            ('coding_exercise', 'Coding Exercise'),
            ('explanation', 'Explanation'),
        ],
        default='multiple_choice'
    )
    
    # Answer options and solutions
    answer_options = JSONField(
        default=list,
        blank=True,
        help_text="List of answer options for multiple choice questions"
    )
    correct_answer = models.TextField(help_text="The correct answer or solution")
    explanation = models.TextField(
        blank=True,
        help_text="Explanation of why this is the correct answer"
    )
    
    # Difficulty and progression
    estimated_difficulty = models.CharField(
        max_length=20,
        choices=[
            ('', 'Not Assigned'),
            ('beginner', 'Beginner'),
            ('easy', 'Easy'),
            ('medium', 'Medium'),
            ('hard', 'Hard'),
            ('expert', 'Expert'),
        ],
        default='beginner',
        blank=True,
        help_text="Difficulty level - starts as 'beginner', can be updated later"
    )
    
    # Game type classification
    game_type = models.CharField(
        max_length=20,
        choices=[
            ('coding', 'Coding Exercise'),
            ('non_coding', 'Non-Coding Question'),
        ],
        default='non_coding',
        help_text="Type of game - coding or non-coding"
    )
    
    # Specific minigame type
    minigame_type = models.CharField(
        max_length=30,
        choices=[
            ('hangman_coding', 'Hangman-Style Coding Game'),
            ('ship_debugging', 'Ship Debugging Game'),
            ('word_search', 'Word Search Puzzle'),
            ('crossword', 'Crossword Puzzle'),
            ('generic', 'Generic Question'),
        ],
        default='generic',
        help_text="Specific minigame type for targeted content generation"
    )
    
    # RAG source tracking
    source_chunks = models.ManyToManyField(
        DocumentChunk,
        blank=True,
        help_text="Chunks used by RAG to generate this question"
    )
    rag_context = JSONField(
        default=dict,
        blank=True,
        help_text="Context and metadata from RAG retrieval"
    )
    
    # Generation metadata
    generation_model = models.CharField(
        max_length=100,
        blank=True,
        help_text="Model used to generate this question (e.g., gpt-4, claude-3)"
    )
    generation_prompt = models.TextField(
        blank=True,
        help_text="Prompt used to generate this question"
    )
    generation_metadata = JSONField(
        default=dict,
        blank=True,
        help_text="Additional metadata about question generation"
    )
    
    # Minigame-specific data
    game_data = JSONField(
        default=dict,
        blank=True,
        help_text="Minigame-specific data like function parameters, validation settings, etc."
    )
    
    # Quality and validation
    quality_score = models.FloatField(
        null=True,
        blank=True,
        help_text="Quality score from validation (0.0 - 1.0)"
    )
    validation_status = models.CharField(
        max_length=20,
        choices=[
            ('pending', 'Pending Validation'),
            ('approved', 'Approved'),
            ('rejected', 'Rejected'),
            ('needs_review', 'Needs Review'),
        ],
        default='pending'
    )
    
    # Timestamps
    created_at = models.DateTimeField(auto_now_add=True)
    updated_at = models.DateTimeField(auto_now=True)
    
    # Usage tracking
    times_used = models.IntegerField(default=0)
    success_rate = models.FloatField(
        null=True,
        blank=True,
        help_text="Success rate when used in practice (0.0 - 1.0)"
    )
    
    class Meta:
        ordering = ['-created_at']
        indexes = [
            models.Index(fields=['subtopic', 'estimated_difficulty']),
            models.Index(fields=['validation_status', 'quality_score']),
            models.Index(fields=['topic', 'question_type']),
        ]
    
    def __str__(self):
        difficulty_str = f" ({self.estimated_difficulty})" if self.estimated_difficulty else ""
        return f"{self.subtopic.name}: {self.question_text[:50]}...{difficulty_str}"


class RAGSession(models.Model):
    """
    Tracks RAG retrieval sessions for question generation.
    """
    # Target for question generation
    subtopic = models.ForeignKey(Subtopic, on_delete=models.CASCADE, related_name='rag_sessions')
    
    # RAG configuration
    query_text = models.TextField(help_text="Query used for RAG retrieval")
    embedding_model = models.CharField(
        max_length=100,
        help_text="Embedding model used for retrieval"
    )
    retrieval_params = JSONField(
        default=dict,
        help_text="Parameters used for retrieval (top_k, similarity_threshold, etc.)"
    )
    
    # Retrieved content
    retrieved_chunks = models.ManyToManyField(
        DocumentChunk,
        through='ChunkRetrievalScore',
        help_text="Chunks retrieved by RAG"
    )
    context_window = models.TextField(
        help_text="Final context window sent to LLM"
    )
    
    # Session results
    questions_generated = models.IntegerField(default=0)
    generation_success = models.BooleanField(default=False)
    error_message = models.TextField(blank=True)
    
    # Metadata
    session_metadata = JSONField(
        default=dict,
        blank=True,
        help_text="Additional session information"
    )
    
    # Timestamps
    started_at = models.DateTimeField(auto_now_add=True)
    completed_at = models.DateTimeField(null=True, blank=True)
    
    class Meta:
        ordering = ['-started_at']
    
    def __str__(self):
        status = "✓" if self.generation_success else "✗"
        return f"{status} RAG for {self.subtopic.name} - {self.questions_generated} questions"


class ChunkRetrievalScore(models.Model):
    """
    Through model for RAG session chunk retrieval with similarity scores.
    """
    rag_session = models.ForeignKey(RAGSession, on_delete=models.CASCADE)
    chunk = models.ForeignKey(DocumentChunk, on_delete=models.CASCADE)
    
    similarity_score = models.FloatField(help_text="Cosine similarity score")
    rank = models.IntegerField(help_text="Rank in retrieval results (1 = highest)")
    
    # Chunk context
    chunk_context = JSONField(
        default=dict,
        blank=True,
        help_text="Additional context about why this chunk was retrieved"
    )
    
    class Meta:
        ordering = ['rank']
        unique_together = [['rag_session', 'chunk']]
    
    def __str__(self):
        return f"Rank {self.rank}: {self.chunk.topic_title} (score: {self.similarity_score:.3f})"


class QuestionGenerationTask(models.Model):
    """
    Tracks batch question generation tasks.
    """
    # Target configuration
    subtopics = models.ManyToManyField(
        Subtopic,
        help_text="Subtopics to generate questions for"
    )
    questions_per_subtopic = models.IntegerField(
        default=5,
        help_text="Number of questions to generate per subtopic"
    )
    
    # Generation parameters
    generation_config = JSONField(
        default=dict,
        help_text="Configuration for question generation (models, prompts, etc.)"
    )
    
    # Task status
    status = models.CharField(
        max_length=20,
        choices=[
            ('pending', 'Pending'),
            ('running', 'Running'),
            ('completed', 'Completed'),
            ('failed', 'Failed'),
            ('cancelled', 'Cancelled'),
        ],
        default='pending'
    )
    
    # Progress tracking
    total_subtopics = models.IntegerField(default=0)
    completed_subtopics = models.IntegerField(default=0)
    total_questions_generated = models.IntegerField(default=0)
    failed_subtopics = models.IntegerField(default=0)
    
    # Results
    task_results = JSONField(
        default=dict,
        blank=True,
        help_text="Detailed results and statistics"
    )
    error_log = JSONField(
        default=list,
        blank=True,
        help_text="List of errors encountered during generation"
    )
    
    # Timestamps
    created_at = models.DateTimeField(auto_now_add=True)
    started_at = models.DateTimeField(null=True, blank=True)
    completed_at = models.DateTimeField(null=True, blank=True)
    
    class Meta:
        ordering = ['-created_at']
    
    def __str__(self):
        return f"Task {self.id}: {self.completed_subtopics}/{self.total_subtopics} subtopics ({self.status})"
    
    @property
    def progress_percentage(self):
        if self.total_subtopics == 0:
            return 0
        return (self.completed_subtopics / self.total_subtopics) * 100
